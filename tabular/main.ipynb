{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, FunctionTransformer\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# # Define the directory containing the CSV files\n",
    "# directory = 'C:/Users/abdul/Documents/SuperProfMLAssignment'  # Change this to the path of your directory\n",
    "\n",
    "# # Get a list of all CSV filenames in the directory\n",
    "# csvfilenames = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# dfs = []\n",
    "\n",
    "# for filename in csvfilenames:\n",
    "#     file_path = os.path.join(directory, filename)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     patient_id = os.path.splitext(filename)[0]  # Extract patient_id from filename\n",
    "#     df['patient_id'] = int(patient_id)  # Ensure patient_id is an integer\n",
    "#     dfs.append(df)\n",
    "\n",
    "# label_df = pd.read_excel('lable.xlsx')\n",
    "# # Ensure patient_id in the label dataframe is an integer\n",
    "# label_df['id_random'] = label_df['id_random'].astype(int)\n",
    "# # Merge each dataframe in dfs with the label dataframe\n",
    "# merged_dfs = []\n",
    "# for df in dfs:\n",
    "#     merged_df = df.merge(label_df, how='left', left_on='patient_id', right_on='id_random')\n",
    "#     # Reorder columns to have patient_id as the first column\n",
    "#     columns_order = ['patient_id'] + [col for col in merged_df.columns if col != 'id_random' and col != 'patient_id']\n",
    "#     merged_df = merged_df[columns_order]\n",
    "#     merged_dfs.append(merged_df)\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# # Apply Transformations\n",
    "# df['FSC_log'] = np.log1p(df['FSC'])\n",
    "# df['SSC_log'] = np.log1p(df['SSC'])\n",
    "# df['CD16_log'] = np.log1p(df['CD16 AF488'])\n",
    "# df['CD14_log'] = np.log1p(df['CD14-PE'])\n",
    "# df['CD42a_log'] = np.log1p(df['CD42a-PerCP'])\n",
    "# df['CCR2_log'] = np.log1p(df['CCR2-APC'])\n",
    "\n",
    "# # Statistical Features \n",
    "# agg_functions = ['mean', 'std', 'median']\n",
    "# df_agg = df.groupby('patient_id')[['FSC_log', 'SSC_log', 'CD16_log', 'CD14_log', 'CD42a_log', 'CCR2_log']].agg(agg_functions)\n",
    "\n",
    "\n",
    "\n",
    "# #Flatten the multi-index columns\n",
    "# df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "\n",
    "# #Add back the patient ID as a column\n",
    "# # df_agg = df_agg.reset_index()\n",
    "\n",
    "# # df_agg = df_agg.merge(label_df, how='left', left_on='patient_id', right_on='id_random')\n",
    "\n",
    "# # df_agg.drop(columns=['id_random'], inplace=True)\n",
    "\n",
    "# # Combine the aggregated features with the original DataFrame\n",
    "# df = pd.concat([df, df_agg], axis=1)\n",
    "\n",
    "# # Remove original features\n",
    "# df.drop(columns=['FSC', 'SSC', 'CD16 AF488', 'CD14-PE', 'CD42a-PerCP', 'CCR2-APC'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Handle infinite and large values\n",
    "# df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infinities with NaN\n",
    "# df.fillna(df.mean(), inplace=True)  # Fill NaNs with mean values\n",
    "\n",
    "# # Cap extremely large values (e.g., 3 standard deviations from the mean)\n",
    "# numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# numeric_cols = numeric_cols.drop(['patient_id'])  # Exclude 'patient_id' from numeric columns\n",
    "# for col in numeric_cols:\n",
    "#     col_mean = df[col].mean()\n",
    "#     col_std = df[col].std()\n",
    "#     cap = col_mean + 3 * col_std\n",
    "#     df[col] = np.where(df[col] > cap, cap, df[col])\n",
    "\n",
    "# # Feature Selection (Remove highly skewed features)\n",
    "# skew_threshold = 2\n",
    "# numeric_features = df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "# skewed_features = numeric_features.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "# skewed_features = skewed_features[skewed_features > skew_threshold]\n",
    "# df.drop(skewed_features.index, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# df.head()\n",
    "# #df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(y_true, y_pred):\n",
    "#     alpha = 0.25\n",
    "#     gamma = 2.0\n",
    "\n",
    "#     p = 1 / (1 + np.exp(-y_pred))\n",
    "#     grad = alpha * (y_true - p) * (1 - p) ** gamma\n",
    "#     hess = alpha * gamma * (y_true - p) * (1 - p) ** (gamma - 1) * p * (1 - p) + grad * (1 - p)\n",
    "    \n",
    "#     return grad, hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    " \n",
    "# # Separate features and target\n",
    "# X = df.drop(columns=['Label', 'patient_id'])  \n",
    "# y = df['Label']\n",
    "# patient_ids = df['patient_id']\n",
    "\n",
    "\n",
    "# # Split data into train, validation, and test sets\n",
    "# X_temp, X_test, y_temp, y_test, patient_ids_temp, patient_ids_test = train_test_split(\n",
    "#     X, y, patient_ids, test_size=0.2, random_state=42, stratify=patient_ids)\n",
    "\n",
    "# X_train, X_val, y_train, y_val, patient_ids_train, patient_ids_val = train_test_split(\n",
    "#     X_temp, y_temp, patient_ids_temp, test_size=0.25, random_state=42, stratify=patient_ids_temp)\n",
    "\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Train the XGBoost classifier\n",
    "# model = XGBClassifier(objective=focal_loss, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "\n",
    "# # Get feature importances\n",
    "# feature_importances = model.feature_importances_\n",
    "# important_feature = X.columns[np.argmax(feature_importances)]\n",
    "\n",
    "# print(f\"Most important feature: {important_feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to select the most significant record for each patient_id\n",
    "# def select_significant_record(group, important_feature):\n",
    "#     # Select the record with the maximum value of the most important feature\n",
    "#     significant_record = group.loc[group[important_feature].idxmax()]\n",
    "#     return significant_record\n",
    "\n",
    "# # Apply the function to each patient_id group\n",
    "# df = df.groupby('patient_id').apply(lambda group: select_significant_record(group, important_feature))\n",
    "\n",
    "# # Reset index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Display the significant records\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>Label</th>\n",
       "      <th>FSC_log</th>\n",
       "      <th>SSC_log</th>\n",
       "      <th>CD16_log</th>\n",
       "      <th>CD14_log</th>\n",
       "      <th>CD42a_log</th>\n",
       "      <th>CCR2_log</th>\n",
       "      <th>FSC_log_mean</th>\n",
       "      <th>FSC_log_std</th>\n",
       "      <th>...</th>\n",
       "      <th>CD16_log_median</th>\n",
       "      <th>CD14_log_mean</th>\n",
       "      <th>CD14_log_std</th>\n",
       "      <th>CD14_log_median</th>\n",
       "      <th>CD42a_log_mean</th>\n",
       "      <th>CD42a_log_std</th>\n",
       "      <th>CD42a_log_median</th>\n",
       "      <th>CCR2_log_mean</th>\n",
       "      <th>CCR2_log_std</th>\n",
       "      <th>CCR2_log_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.492240</td>\n",
       "      <td>6.373320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.929589</td>\n",
       "      <td>6.334254</td>\n",
       "      <td>0.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.385174</td>\n",
       "      <td>5.289046</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>5.451764</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>5.18782</td>\n",
       "      <td>5.636373</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>5.746276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.563856</td>\n",
       "      <td>6.529419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.505332</td>\n",
       "      <td>5.852202</td>\n",
       "      <td>6.334254</td>\n",
       "      <td>0.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.385174</td>\n",
       "      <td>5.289046</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>5.451764</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>5.18782</td>\n",
       "      <td>5.636373</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>5.746276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.896694</td>\n",
       "      <td>6.434547</td>\n",
       "      <td>4.867534</td>\n",
       "      <td>5.616771</td>\n",
       "      <td>5.480639</td>\n",
       "      <td>6.334254</td>\n",
       "      <td>0.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.385174</td>\n",
       "      <td>5.289046</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>5.451764</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>5.18782</td>\n",
       "      <td>5.636373</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>5.746276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>6.428105</td>\n",
       "      <td>6.760415</td>\n",
       "      <td>5.049856</td>\n",
       "      <td>6.291569</td>\n",
       "      <td>6.334254</td>\n",
       "      <td>0.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.385174</td>\n",
       "      <td>5.289046</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>5.451764</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>5.18782</td>\n",
       "      <td>5.636373</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>5.746276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.931472</td>\n",
       "      <td>6.520621</td>\n",
       "      <td>6.476972</td>\n",
       "      <td>6.670766</td>\n",
       "      <td>6.508769</td>\n",
       "      <td>6.334254</td>\n",
       "      <td>0.539625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.385174</td>\n",
       "      <td>5.289046</td>\n",
       "      <td>0.902717</td>\n",
       "      <td>5.451764</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>5.18782</td>\n",
       "      <td>5.636373</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>5.746276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  Label   FSC_log   SSC_log  CD16_log  CD14_log  CD42a_log  \\\n",
       "0       103.0    0.0  6.931472  6.931472  6.492240  6.373320   0.000000   \n",
       "1       105.0    0.0  6.931472  6.563856  6.529419  0.000000   5.505332   \n",
       "2       110.0    0.0  6.931472  6.896694  6.434547  4.867534   5.616771   \n",
       "3       113.0    0.0  6.931472  5.192957  6.428105  6.760415   5.049856   \n",
       "4       116.0    0.0  6.931472  6.931472  6.520621  6.476972   6.670766   \n",
       "\n",
       "   CCR2_log  FSC_log_mean  FSC_log_std  ...  CD16_log_median  CD14_log_mean  \\\n",
       "0  5.929589      6.334254     0.539625  ...         6.385174       5.289046   \n",
       "1  5.852202      6.334254     0.539625  ...         6.385174       5.289046   \n",
       "2  5.480639      6.334254     0.539625  ...         6.385174       5.289046   \n",
       "3  6.291569      6.334254     0.539625  ...         6.385174       5.289046   \n",
       "4  6.508769      6.334254     0.539625  ...         6.385174       5.289046   \n",
       "\n",
       "   CD14_log_std  CD14_log_median  CD42a_log_mean  CD42a_log_std  \\\n",
       "0      0.902717         5.451764        4.975513        0.96248   \n",
       "1      0.902717         5.451764        4.975513        0.96248   \n",
       "2      0.902717         5.451764        4.975513        0.96248   \n",
       "3      0.902717         5.451764        4.975513        0.96248   \n",
       "4      0.902717         5.451764        4.975513        0.96248   \n",
       "\n",
       "   CD42a_log_median  CCR2_log_mean  CCR2_log_std  CCR2_log_median  \n",
       "0           5.18782       5.636373      0.585351         5.746276  \n",
       "1           5.18782       5.636373      0.585351         5.746276  \n",
       "2           5.18782       5.636373      0.585351         5.746276  \n",
       "3           5.18782       5.636373      0.585351         5.746276  \n",
       "4           5.18782       5.636373      0.585351         5.746276  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nada1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 24)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['Label', 'patient_id']).values  \n",
    "df['Label'] = df['Label'].astype(int)\n",
    "y = df['Label'].values\n",
    "patient_ids = df['patient_id'].values\n",
    "print(X.shape)\n",
    "\n",
    "# # Split data into train and remaining (val + test)\n",
    "# X_train, X_remaining, y_train, y_remaining, patient_ids_train, patient_ids_remaining = train_test_split(\n",
    "#     X, y, patient_ids, test_size=0.4, random_state=42  # Reserve 40% for val + test\n",
    "# )\n",
    "\n",
    "# # Split the remaining data into validation and test sets of equal size\n",
    "# X_val, X_test, y_val, y_test, patient_ids_val, patient_ids_test = train_test_split(\n",
    "#     X_remaining, y_remaining, patient_ids_remaining, test_size=0.5, random_state=42  # 50% of remaining -> 20% of original\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return torch.mean(F_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n",
    "        super().__init__()\n",
    "       \n",
    "\n",
    "        hidden_size = sign_size * cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size // 2\n",
    "        output_size = (sign_size // 4) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input * K, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input * K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input * K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_hidden)\n",
    "        conv3 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv3 = nn.utils.weight_norm(conv3, dim=None)\n",
    "\n",
    "        # 4th conv layer\n",
    "        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n",
    "        conv4 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2, \n",
    "            groups=cha_hidden, \n",
    "            bias=False)\n",
    "        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n",
    "\n",
    "        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm1d(output_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_output)\n",
    "        dense2 = nn.Linear(output_size, output_dim, bias=False)\n",
    "        self.dense2 = nn.utils.weight_norm(dense2)\n",
    "\n",
    "        self.loss = FocalLoss(alpha=0.75, gamma=2)\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        \n",
    "\n",
    "        self.train_accuracy = Accuracy(task='binary')\n",
    "        self.val_accuracy = Accuracy(task='binary')\n",
    "        self.test_accuracy = Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = self.batch_norm_c4(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x + x_s\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.avg_po_c4(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "    \n",
    "        loss = self.loss(y_hat, y.float())\n",
    "        preds = torch.sigmoid(y_hat)\n",
    "        acc = self.train_accuracy(preds, y.int())\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.loss(y_hat, y.float())\n",
    "        preds = torch.sigmoid(y_hat)\n",
    "        acc = self.val_accuracy(preds, y.int())\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_accuracy', self.test_accuracy(y_hat, y.int()), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results\n",
    "def aggregate_results(results):\n",
    "    avg_report = {}\n",
    "    for key in results[0].keys():\n",
    "        if isinstance(results[0][key], dict):\n",
    "            avg_report[key] = {}\n",
    "            for sub_key in results[0][key].keys():\n",
    "                avg_report[key][sub_key] = np.mean([result[key][sub_key] for result in results])\n",
    "        else:\n",
    "            avg_report[key] = np.mean([result[key] for result in results])\n",
    "    return avg_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1.1:\n",
      "  Training samples: 147\n",
      "  Validation samples: 49\n",
      "  Testing samples: 50\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.71it/s, v_num=547, train_loss=0.537, train_acc=0.737, val_loss=0.667, val_acc=0.653]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 27.13it/s, v_num=547, train_loss=0.537, train_acc=0.737, val_loss=0.667, val_acc=0.653]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 83.34it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5799999833106995     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6658413410186768     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5799999833106995    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6658413410186768    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[29 10]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.6530612244897959\n",
      "Classification Report: {'0': {'precision': 0.8055555555555556, 'recall': 0.7435897435897436, 'f1-score': 0.7733333333333334, 'support': 39}, '1': {'precision': 0.23076923076923078, 'recall': 0.3, 'f1-score': 0.2608695652173913, 'support': 10}, 'accuracy': 0.6530612244897959, 'macro avg': {'precision': 0.5181623931623932, 'recall': 0.5217948717948718, 'f1-score': 0.5171014492753624, 'support': 49}, 'weighted avg': {'precision': 0.6882522239665098, 'recall': 0.6530612244897959, 'f1-score': 0.6687488908606922, 'support': 49}}\n",
      "Fold 1.2:\n",
      "  Training samples: 147\n",
      "  Validation samples: 49\n",
      "  Testing samples: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 34.01it/s, v_num=548, train_loss=0.569, train_acc=0.737, val_loss=0.682, val_acc=0.694]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.67it/s, v_num=548, train_loss=0.569, train_acc=0.737, val_loss=0.682, val_acc=0.694]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 76.93it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7200000286102295     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6202609539031982     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7200000286102295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6202609539031982    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[34  5]\n",
      " [10  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6938775510204082\n",
      "Classification Report: {'0': {'precision': 0.7727272727272727, 'recall': 0.8717948717948718, 'f1-score': 0.8192771084337349, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'accuracy': 0.6938775510204082, 'macro avg': {'precision': 0.38636363636363635, 'recall': 0.4358974358974359, 'f1-score': 0.40963855421686746, 'support': 49}, 'weighted avg': {'precision': 0.6150278293135436, 'recall': 0.6938775510204082, 'f1-score': 0.6520776985492992, 'support': 49}}\n",
      "Fold 1.3:\n",
      "  Training samples: 147\n",
      "  Validation samples: 49\n",
      "  Testing samples: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.87it/s, v_num=549, train_loss=0.592, train_acc=0.737, val_loss=0.645, val_acc=0.714]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.74it/s, v_num=549, train_loss=0.592, train_acc=0.737, val_loss=0.645, val_acc=0.714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 83.34it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7599999904632568     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6226770877838135     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7599999904632568    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6226770877838135    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[35  4]\n",
      " [10  0]]\n",
      "Accuracy: 0.7142857142857143\n",
      "Classification Report: {'0': {'precision': 0.7777777777777778, 'recall': 0.8974358974358975, 'f1-score': 0.8333333333333333, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.3888888888888889, 'recall': 0.44871794871794873, 'f1-score': 0.41666666666666663, 'support': 49}, 'weighted avg': {'precision': 0.6190476190476191, 'recall': 0.7142857142857143, 'f1-score': 0.6632653061224489, 'support': 49}}\n",
      "Fold 1.4:\n",
      "  Training samples: 147\n",
      "  Validation samples: 49\n",
      "  Testing samples: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 124.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 27.85it/s, v_num=550, train_loss=0.647, train_acc=0.632, val_loss=0.631, val_acc=0.653]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 25.44it/s, v_num=550, train_loss=0.647, train_acc=0.632, val_loss=0.631, val_acc=0.653]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 72.68it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6800000071525574     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6328441500663757     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6800000071525574    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6328441500663757    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[31  8]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6530612244897959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.775, 'recall': 0.7948717948717948, 'f1-score': 0.7848101265822786, 'support': 39}, '1': {'precision': 0.1111111111111111, 'recall': 0.1, 'f1-score': 0.10526315789473685, 'support': 10}, 'accuracy': 0.6530612244897959, 'macro avg': {'precision': 0.44305555555555554, 'recall': 0.4474358974358974, 'f1-score': 0.4450366422385077, 'support': 49}, 'weighted avg': {'precision': 0.6395124716553289, 'recall': 0.6530612244897959, 'f1-score': 0.6461270717480864, 'support': 49}}\n",
      "Fold 2.1:\n",
      "  Training samples: 147\n",
      "  Validation samples: 50\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.36it/s, v_num=551, train_loss=0.566, train_acc=0.789, val_loss=0.604, val_acc=0.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.50it/s, v_num=551, train_loss=0.566, train_acc=0.789, val_loss=0.604, val_acc=0.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 75.42it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5874741077423096     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5874741077423096    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[36  3]\n",
      " [11  0]]\n",
      "Accuracy: 0.72\n",
      "Classification Report: {'0': {'precision': 0.7659574468085106, 'recall': 0.9230769230769231, 'f1-score': 0.8372093023255814, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11}, 'accuracy': 0.72, 'macro avg': {'precision': 0.3829787234042553, 'recall': 0.46153846153846156, 'f1-score': 0.4186046511627907, 'support': 50}, 'weighted avg': {'precision': 0.5974468085106382, 'recall': 0.72, 'f1-score': 0.6530232558139535, 'support': 50}}\n",
      "Fold 2.2:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 31.25it/s, v_num=552, train_loss=0.572, train_acc=0.750, val_loss=0.619, val_acc=0.755]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 28.41it/s, v_num=552, train_loss=0.572, train_acc=0.750, val_loss=0.619, val_acc=0.755]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 88.85it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5938753485679626     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5938753485679626    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[37  2]\n",
      " [10  0]]\n",
      "Accuracy: 0.7551020408163265\n",
      "Classification Report: {'0': {'precision': 0.7872340425531915, 'recall': 0.9487179487179487, 'f1-score': 0.8604651162790696, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'accuracy': 0.7551020408163265, 'macro avg': {'precision': 0.39361702127659576, 'recall': 0.47435897435897434, 'f1-score': 0.4302325581395348, 'support': 49}, 'weighted avg': {'precision': 0.6265740338688667, 'recall': 0.7551020408163265, 'f1-score': 0.6848599905078309, 'support': 49}}\n",
      "Fold 2.3:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 111.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 33.89it/s, v_num=553, train_loss=0.671, train_acc=0.600, val_loss=0.598, val_acc=0.735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.76it/s, v_num=553, train_loss=0.671, train_acc=0.600, val_loss=0.598, val_acc=0.735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 76.92it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6938775777816772     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6637728810310364     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6938775777816772    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6637728810310364    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[34  5]\n",
      " [ 8  2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7346938775510204\n",
      "Classification Report: {'0': {'precision': 0.8095238095238095, 'recall': 0.8717948717948718, 'f1-score': 0.8395061728395062, 'support': 39}, '1': {'precision': 0.2857142857142857, 'recall': 0.2, 'f1-score': 0.23529411764705882, 'support': 10}, 'accuracy': 0.7346938775510204, 'macro avg': {'precision': 0.5476190476190477, 'recall': 0.5358974358974359, 'f1-score': 0.5374001452432825, 'support': 49}, 'weighted avg': {'precision': 0.7026239067055394, 'recall': 0.7346938775510204, 'f1-score': 0.71619759014717, 'support': 49}}\n",
      "Fold 2.4:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 124.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 33.33it/s, v_num=554, train_loss=0.644, train_acc=0.600, val_loss=0.666, val_acc=0.612]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.94it/s, v_num=554, train_loss=0.644, train_acc=0.600, val_loss=0.666, val_acc=0.612]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 74.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5918367505073547     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6677460670471191     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5918367505073547    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6677460670471191    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[26 13]\n",
      " [ 6  4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6122448979591837\n",
      "Classification Report: {'0': {'precision': 0.8125, 'recall': 0.6666666666666666, 'f1-score': 0.7323943661971831, 'support': 39}, '1': {'precision': 0.23529411764705882, 'recall': 0.4, 'f1-score': 0.29629629629629634, 'support': 10}, 'accuracy': 0.6122448979591837, 'macro avg': {'precision': 0.5238970588235294, 'recall': 0.5333333333333333, 'f1-score': 0.5143453312467398, 'support': 49}, 'weighted avg': {'precision': 0.6947028811524609, 'recall': 0.6122448979591837, 'f1-score': 0.6433947600949612, 'support': 49}}\n",
      "Fold 3.1:\n",
      "  Training samples: 147\n",
      "  Validation samples: 50\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 90.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 33.56it/s, v_num=555, train_loss=0.673, train_acc=0.632, val_loss=0.733, val_acc=0.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.30it/s, v_num=555, train_loss=0.673, train_acc=0.632, val_loss=0.733, val_acc=0.420]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 76.92it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4285714328289032     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7072157859802246     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4285714328289032    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7072157859802246    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[16 23]\n",
      " [ 6  5]]\n",
      "Accuracy: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7272727272727273, 'recall': 0.41025641025641024, 'f1-score': 0.5245901639344261, 'support': 39}, '1': {'precision': 0.17857142857142858, 'recall': 0.45454545454545453, 'f1-score': 0.25641025641025644, 'support': 11}, 'accuracy': 0.42, 'macro avg': {'precision': 0.45292207792207795, 'recall': 0.43240093240093236, 'f1-score': 0.39050021017234127, 'support': 50}, 'weighted avg': {'precision': 0.6065584415584415, 'recall': 0.42, 'f1-score': 0.4655905842791088, 'support': 50}}\n",
      "Fold 3.2:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 111.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.88it/s, v_num=556, train_loss=0.564, train_acc=0.700, val_loss=0.626, val_acc=0.694]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.75it/s, v_num=556, train_loss=0.564, train_acc=0.700, val_loss=0.626, val_acc=0.694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 76.94it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6938775777816772     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5815407037734985     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6938775777816772    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5815407037734985    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[33  6]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6938775510204082\n",
      "Classification Report: {'0': {'precision': 0.7857142857142857, 'recall': 0.8461538461538461, 'f1-score': 0.8148148148148148, 'support': 39}, '1': {'precision': 0.14285714285714285, 'recall': 0.1, 'f1-score': 0.11764705882352941, 'support': 10}, 'accuracy': 0.6938775510204082, 'macro avg': {'precision': 0.4642857142857143, 'recall': 0.47307692307692306, 'f1-score': 0.4662309368191721, 'support': 49}, 'weighted avg': {'precision': 0.6545189504373178, 'recall': 0.6938775510204082, 'f1-score': 0.6725356809390423, 'support': 49}}\n",
      "Fold 3.3:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 111.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.15it/s, v_num=557, train_loss=0.668, train_acc=0.600, val_loss=0.608, val_acc=0.714]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.32it/s, v_num=557, train_loss=0.668, train_acc=0.600, val_loss=0.608, val_acc=0.714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 71.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.604304850101471     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.604304850101471    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[34  5]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7906976744186046, 'recall': 0.8717948717948718, 'f1-score': 0.8292682926829267, 'support': 39}, '1': {'precision': 0.16666666666666666, 'recall': 0.1, 'f1-score': 0.125, 'support': 10}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.4786821705426356, 'recall': 0.4858974358974359, 'f1-score': 0.47713414634146334, 'support': 49}, 'weighted avg': {'precision': 0.663344407530454, 'recall': 0.7142857142857143, 'f1-score': 0.685540069686411, 'support': 49}}\n",
      "Fold 3.4:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 105.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.67it/s, v_num=558, train_loss=0.690, train_acc=0.550, val_loss=0.658, val_acc=0.714]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.76it/s, v_num=558, train_loss=0.690, train_acc=0.550, val_loss=0.658, val_acc=0.714]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 71.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6046683192253113     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6046683192253113    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[33  6]\n",
      " [ 8  2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Classification Report: {'0': {'precision': 0.8048780487804879, 'recall': 0.8461538461538461, 'f1-score': 0.8250000000000001, 'support': 39}, '1': {'precision': 0.25, 'recall': 0.2, 'f1-score': 0.22222222222222224, 'support': 10}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.5274390243902439, 'recall': 0.5230769230769231, 'f1-score': 0.5236111111111111, 'support': 49}, 'weighted avg': {'precision': 0.6916376306620209, 'recall': 0.7142857142857143, 'f1-score': 0.701984126984127, 'support': 49}}\n",
      "Fold 4.1:\n",
      "  Training samples: 147\n",
      "  Validation samples: 50\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 90.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.47it/s, v_num=559, train_loss=0.598, train_acc=0.632, val_loss=0.611, val_acc=0.740]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.41it/s, v_num=559, train_loss=0.598, train_acc=0.632, val_loss=0.611, val_acc=0.740]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 74.06it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.795918345451355     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5875157117843628     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.795918345451355    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5875157117843628    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[37  2]\n",
      " [11  0]]\n",
      "Accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7708333333333334, 'recall': 0.9487179487179487, 'f1-score': 0.8505747126436781, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11}, 'accuracy': 0.74, 'macro avg': {'precision': 0.3854166666666667, 'recall': 0.47435897435897434, 'f1-score': 0.42528735632183906, 'support': 50}, 'weighted avg': {'precision': 0.60125, 'recall': 0.74, 'f1-score': 0.6634482758620689, 'support': 50}}\n",
      "Fold 4.2:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 33.33it/s, v_num=560, train_loss=0.588, train_acc=0.700, val_loss=0.634, val_acc=0.592]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.12it/s, v_num=560, train_loss=0.588, train_acc=0.700, val_loss=0.634, val_acc=0.592]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 66.67it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7142857313156128     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5962436199188232     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7142857313156128    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5962436199188232    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[27 12]\n",
      " [ 8  2]]\n",
      "Accuracy: 0.5918367346938775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7714285714285715, 'recall': 0.6923076923076923, 'f1-score': 0.7297297297297296, 'support': 39}, '1': {'precision': 0.14285714285714285, 'recall': 0.2, 'f1-score': 0.16666666666666666, 'support': 10}, 'accuracy': 0.5918367346938775, 'macro avg': {'precision': 0.4571428571428572, 'recall': 0.4461538461538461, 'f1-score': 0.4481981981981981, 'support': 49}, 'weighted avg': {'precision': 0.6431486880466472, 'recall': 0.5918367346938775, 'f1-score': 0.6148189005331862, 'support': 49}}\n",
      "Fold 4.3:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 99.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.58it/s, v_num=561, train_loss=0.671, train_acc=0.700, val_loss=0.675, val_acc=0.694]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 27.85it/s, v_num=561, train_loss=0.671, train_acc=0.700, val_loss=0.675, val_acc=0.694]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 83.35it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7142857313156128     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.643415629863739     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7142857313156128    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.643415629863739    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[33  6]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6938775510204082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7857142857142857, 'recall': 0.8461538461538461, 'f1-score': 0.8148148148148148, 'support': 39}, '1': {'precision': 0.14285714285714285, 'recall': 0.1, 'f1-score': 0.11764705882352941, 'support': 10}, 'accuracy': 0.6938775510204082, 'macro avg': {'precision': 0.4642857142857143, 'recall': 0.47307692307692306, 'f1-score': 0.4662309368191721, 'support': 49}, 'weighted avg': {'precision': 0.6545189504373178, 'recall': 0.6938775510204082, 'f1-score': 0.6725356809390423, 'support': 49}}\n",
      "Fold 4.4:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 28.73it/s, v_num=562, train_loss=0.608, train_acc=0.600, val_loss=0.619, val_acc=0.755]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 26.45it/s, v_num=562, train_loss=0.608, train_acc=0.600, val_loss=0.619, val_acc=0.755]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 80.01it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5981003642082214     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5981003642082214    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[34  5]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.7551020408163265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.8292682926829268, 'recall': 0.8717948717948718, 'f1-score': 0.8500000000000001, 'support': 39}, '1': {'precision': 0.375, 'recall': 0.3, 'f1-score': 0.33333333333333326, 'support': 10}, 'accuracy': 0.7551020408163265, 'macro avg': {'precision': 0.6021341463414633, 'recall': 0.5858974358974359, 'f1-score': 0.5916666666666667, 'support': 49}, 'weighted avg': {'precision': 0.7365604778496764, 'recall': 0.7551020408163265, 'f1-score': 0.7445578231292519, 'support': 49}}\n",
      "Fold 5.1:\n",
      "  Training samples: 147\n",
      "  Validation samples: 50\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 33.55it/s, v_num=563, train_loss=0.432, train_acc=0.947, val_loss=0.556, val_acc=0.760]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.11it/s, v_num=563, train_loss=0.432, train_acc=0.947, val_loss=0.556, val_acc=0.760]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 74.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5724124908447266     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5724124908447266    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[38  1]\n",
      " [11  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Classification Report: {'0': {'precision': 0.7755102040816326, 'recall': 0.9743589743589743, 'f1-score': 0.8636363636363635, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11}, 'accuracy': 0.76, 'macro avg': {'precision': 0.3877551020408163, 'recall': 0.48717948717948717, 'f1-score': 0.43181818181818177, 'support': 50}, 'weighted avg': {'precision': 0.6048979591836734, 'recall': 0.76, 'f1-score': 0.6736363636363636, 'support': 50}}\n",
      "Fold 5.2:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 90.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 31.34it/s, v_num=564, train_loss=0.633, train_acc=0.750, val_loss=0.672, val_acc=0.571]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 28.49it/s, v_num=564, train_loss=0.633, train_acc=0.750, val_loss=0.672, val_acc=0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7551020383834839     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6235777139663696     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7551020383834839    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6235777139663696    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[25 14]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.5714285714285714\n",
      "Classification Report: {'0': {'precision': 0.78125, 'recall': 0.6410256410256411, 'f1-score': 0.7042253521126761, 'support': 39}, '1': {'precision': 0.17647058823529413, 'recall': 0.3, 'f1-score': 0.22222222222222224, 'support': 10}, 'accuracy': 0.5714285714285714, 'macro avg': {'precision': 0.4788602941176471, 'recall': 0.4705128205128205, 'f1-score': 0.46322378716744916, 'support': 49}, 'weighted avg': {'precision': 0.6578256302521008, 'recall': 0.5714285714285715, 'f1-score': 0.6058573664207467, 'support': 49}}\n",
      "Fold 5.3:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 100.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 32.36it/s, v_num=565, train_loss=0.600, train_acc=0.750, val_loss=0.595, val_acc=0.796]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 29.15it/s, v_num=565, train_loss=0.600, train_acc=0.750, val_loss=0.595, val_acc=0.796]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 76.91it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.795918345451355     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5775396227836609     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.795918345451355    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5775396227836609    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[39  0]\n",
      " [10  0]]\n",
      "Accuracy: 0.7959183673469388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: {'0': {'precision': 0.7959183673469388, 'recall': 1.0, 'f1-score': 0.8863636363636364, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'accuracy': 0.7959183673469388, 'macro avg': {'precision': 0.3979591836734694, 'recall': 0.5, 'f1-score': 0.4431818181818182, 'support': 49}, 'weighted avg': {'precision': 0.6334860474802166, 'recall': 0.7959183673469388, 'f1-score': 0.7054730983302412, 'support': 49}}\n",
      "Fold 5.4:\n",
      "  Training samples: 148\n",
      "  Validation samples: 49\n",
      "  Testing samples: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | batch_norm1    | BatchNorm1d       | 48     | train\n",
      "1  | dropout1       | Dropout           | 0      | train\n",
      "2  | dense1         | Linear            | 12.8 K | train\n",
      "3  | batch_norm_c1  | BatchNorm1d       | 32     | train\n",
      "4  | conv1          | Conv1d            | 161    | train\n",
      "5  | ave_po_c1      | AdaptiveAvgPool1d | 0      | train\n",
      "6  | batch_norm_c2  | BatchNorm1d       | 64     | train\n",
      "7  | dropout_c2     | Dropout           | 0      | train\n",
      "8  | conv2          | Conv1d            | 3.1 K  | train\n",
      "9  | batch_norm_c3  | BatchNorm1d       | 64     | train\n",
      "10 | dropout_c3     | Dropout           | 0      | train\n",
      "11 | conv3          | Conv1d            | 3.1 K  | train\n",
      "12 | batch_norm_c4  | BatchNorm1d       | 64     | train\n",
      "13 | conv4          | Conv1d            | 161    | train\n",
      "14 | avg_po_c4      | AvgPool1d         | 0      | train\n",
      "15 | flt            | Flatten           | 0      | train\n",
      "16 | batch_norm2    | BatchNorm1d       | 512    | train\n",
      "17 | dropout2       | Dropout           | 0      | train\n",
      "18 | dense2         | Linear            | 257    | train\n",
      "19 | loss           | BCEWithLogitsLoss | 0      | train\n",
      "20 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "21 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "22 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------------\n",
      "20.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 K    Total params\n",
      "0.081     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 34.25it/s, v_num=566, train_loss=0.553, train_acc=0.800, val_loss=0.590, val_acc=0.776]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 30.86it/s, v_num=566, train_loss=0.553, train_acc=0.800, val_loss=0.590, val_acc=0.776]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 80.00it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7755101919174194     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5775507688522339     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7755101919174194    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5775507688522339    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[38  1]\n",
      " [10  0]]\n",
      "Accuracy: 0.7755102040816326\n",
      "Classification Report: {'0': {'precision': 0.7916666666666666, 'recall': 0.9743589743589743, 'f1-score': 0.8735632183908045, 'support': 39}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'accuracy': 0.7755102040816326, 'macro avg': {'precision': 0.3958333333333333, 'recall': 0.48717948717948717, 'f1-score': 0.43678160919540227, 'support': 49}, 'weighted avg': {'precision': 0.6301020408163265, 'recall': 0.7755102040816326, 'f1-score': 0.6952850105559464, 'support': 49}}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "cms = []  # List to store confusion matrices for each fold\n",
    "accuracies = []\n",
    "\n",
    "for fold_outer, (train_val_idx, test_idx) in enumerate(kfold_outer.split(X, y)):\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    patient_ids_train_val = patient_ids[train_val_idx]\n",
    "    patient_ids_test = patient_ids[test_idx]\n",
    "    kfold_inner = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)  # Inner loop for validation\n",
    "    for fold_inner, (train_idx, val_idx) in enumerate(kfold_inner.split(X_train_val, y_train_val)):\n",
    "        X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "        y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "        patient_ids_train = patient_ids_train_val[train_idx]\n",
    "        patient_ids_val = patient_ids_train_val[val_idx]\n",
    "\n",
    "        print(f\"Fold {fold_outer + 1}.{fold_inner + 1}:\")\n",
    "        print(f\"  Training samples: {len(X_train)}\")\n",
    "        print(f\"  Validation samples: {len(X_val)}\")\n",
    "        print(f\"  Testing samples: {len(X_test)}\")\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train , dtype=torch.float32), torch.tensor(y_train,  dtype=torch.float32))\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val,  dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "        test_dataset = TensorDataset(torch.tensor(X_test , dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "        # Create DataLoaders\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize the model\n",
    "        input_dim = X.shape[1]\n",
    "        output_dim = 1 # Since it is Binary Classification\n",
    "\n",
    "        model = SoftOrdering1DCNN(input_dim, output_dim)\n",
    "\n",
    "        # Initialize the trainer\n",
    "        trainer = pl.Trainer(max_epochs=10, accelerator='gpu' if torch.cuda.is_available() else 'auto')\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader, val_loader )\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        trainer.test(model, test_loader)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x,_ = batch\n",
    "                y_hat = model(x).squeeze()\n",
    "                y_pred.extend(torch.sigmoid(y_hat).cpu().numpy())\n",
    "\n",
    "        # Convert y_pred to binary predictions (0 or 1) based on a threshold (e.g., 0.5)\n",
    "        y_pred = np.round(y_pred).astype(int)\n",
    "        \n",
    "\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        print(f\"Confusion Matrix: {cm}\")\n",
    "        \n",
    "        cms.append(cm)\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate classification metrics\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        print(f\"Classification Report: {report}\")\n",
    "\n",
    "        fold_results.append(report)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_id  true_label  predicted_label\n",
      "0       126.0           0                0\n",
      "1       134.0           0                0\n",
      "2       150.0           0                0\n",
      "3       160.0           1                0\n",
      "4       172.0           0                0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Map predictions back to patient_id\n",
    "results = pd.DataFrame({\n",
    "       'patient_id': patient_ids_val,\n",
    "       'true_label': y_val,\n",
    "       'predicted_label': y_pred  # Ensure the length matches y_val\n",
    "})\n",
    "# Display the results\n",
    "print(results.head())\n",
    "\n",
    "# Save the results to excel file\n",
    "results.to_excel('label.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_classification_report(report):\n",
    "    formatted = []\n",
    "    for class_name, metrics in report.items():\n",
    "        if class_name == 'accuracy':  # Handle overall accuracy separately\n",
    "            formatted.append(['accuracy', '', metrics])\n",
    "        elif class_name == 'macro avg':\n",
    "            formatted.append(['macro avg', '', metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']])\n",
    "        elif class_name == 'weighted avg':\n",
    "            formatted.append(['weighted avg', '', metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']])\n",
    "        else:\n",
    "            formatted.append([int(class_name), metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']])\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "---------------------------------------------------\n",
      "Average cross-validation results:\n",
      " ╒══════════════╤═════════════════════╤══════════╤════════════╤═══════════╕\n",
      "│ class        │ precision           │   recall │   f1-score │   support │\n",
      "╞══════════════╪═════════════════════╪══════════╪════════════╪═══════════╡\n",
      "│ 0            │ 0.7858214181193288  │ 0.832051 │   0.802345 │ 39        │\n",
      "├──────────────┼─────────────────────┼──────────┼────────────┼───────────┤\n",
      "│ 1            │ 0.12190844286432523 │ 0.137727 │   0.122944 │ 10.2      │\n",
      "├──────────────┼─────────────────────┼──────────┼────────────┼───────────┤\n",
      "│ accuracy     │                     │ 0.688122 │            │           │\n",
      "├──────────────┼─────────────────────┼──────────┼────────────┼───────────┤\n",
      "│ macro avg    │                     │ 0.453865 │   0.484889 │  0.462645 │\n",
      "├──────────────┼─────────────────────┼──────────┼────────────┼───────────┤\n",
      "│ weighted avg │                     │ 0.648052 │   0.688122 │  0.661448 │\n",
      "╘══════════════╧═════════════════════╧══════════╧════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "mean_cm = np.round(np.mean(cms, axis=0))\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(mean_cm, annot=True, fmt='.0f', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "# Print other evaluation metrics\n",
    "mean_accuracy = np.round(np.mean(accuracies, axis=0))\n",
    "print(f'Accuracy: {mean_accuracy}')\n",
    "print(\"---------------------------------------------------\")\n",
    "avg_results = aggregate_results(fold_results)\n",
    "formatted_report = format_classification_report(avg_results)\n",
    "print(\"Average cross-validation results:\\n\", tabulate(formatted_report, headers=['class', 'precision', 'recall', 'f1-score', 'support'], tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
